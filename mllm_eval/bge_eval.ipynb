{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from FlagEmbedding.visual.modeling import Visualized_BGE\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import requests\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load(f):\n",
    "    def load_pkl(pth):\n",
    "        return pickle.load(open(pth, 'rb'))\n",
    "\n",
    "    def load_json(pth):\n",
    "        return json.load(open(pth, 'r', encoding='utf-8'))\n",
    "\n",
    "    def load_jsonl(f):\n",
    "        lines = open(f, encoding='utf-8').readlines()\n",
    "        lines = [x.strip() for x in lines]\n",
    "        if lines[-1] == '':\n",
    "            lines = lines[:-1]\n",
    "        data = [json.loads(x) for x in lines]\n",
    "        return data\n",
    "\n",
    "    def load_xlsx(f):\n",
    "        return pd.read_excel(f)\n",
    "\n",
    "    def load_csv(f):\n",
    "        return pd.read_csv(f)\n",
    "\n",
    "    def load_tsv(f):\n",
    "        return pd.read_csv(f, sep='\\t')\n",
    "\n",
    "    handlers = dict(pkl=load_pkl, json=load_json, jsonl=load_jsonl, xlsx=load_xlsx, csv=load_csv, tsv=load_tsv)\n",
    "    suffix = f.split('.')[-1]\n",
    "    return handlers[suffix](f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def eval_single_choice(benchmark_data, benchmark_root_path, model):\n",
    "        \n",
    "    correct = 0\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(benchmark_data.iterrows(), desc=f'Evaluating'):\n",
    "        images_list = []\n",
    "        # Iterate over each column in the row\n",
    "        for column_name in benchmark_data.columns:\n",
    "            # Check if the column name starts with 'img'\n",
    "            if column_name.startswith('img') and not column_name.startswith('url') and not pd.isna(row[column_name]):\n",
    "                images_list.append(benchmark_root_path + '/' + row[column_name])\n",
    "\n",
    "        if pd.isna(row['below_content']): row['below_content'] = ''\n",
    "        if pd.isna(row['above_content']): row['above_content'] = ''\n",
    "        inputs = {\n",
    "            'above_content': row['above_content'],\n",
    "            'below_content': row['below_content'],\n",
    "            'images': images_list,\n",
    "        }\n",
    "\n",
    "        sim_answer_list = []\n",
    "        try:\n",
    "            for img in images_list:\n",
    "                img_emb = model.encode(image=img)\n",
    "                txt_emb = model.encode(text=inputs['above_content'] + inputs['below_content'])\n",
    "                sim_answer = img_emb @ txt_emb.T\n",
    "                sim_answer_list.append(sim_answer.item())\n",
    "            \n",
    "            model_answer = chr(sim_answer_list.index(max(sim_answer_list)) + 65)\n",
    "        except: \n",
    "            model_answer = 'ERROR!'\n",
    "        true_label = row['Answer']\n",
    "        \n",
    "        # Check if the model's answer is correct\n",
    "        if model_answer == true_label:\n",
    "            correct += 1\n",
    "        \n",
    "        # Append the results to the list\n",
    "        result_row = row.to_dict()\n",
    "        result_row['sim_answer_list'] = sim_answer_list\n",
    "        result_row['model_answer'] = model_answer\n",
    "        results.append(result_row)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = correct / len(benchmark_data)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Convert the DataFrame to JSON format\n",
    "    results_json = results_df.to_dict(orient='records')\n",
    "    \n",
    "    # Combine accuracy and results into one dictionary\n",
    "    output = {\n",
    "        'accuracy': accuracy,\n",
    "        'results': results_json\n",
    "    }\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def eval_flow_insert(benchmark_data, benchmark_root_path, model, similarity_threshold=0.43):\n",
    "\n",
    "    def compute_similarity(paragraph, image_path):\n",
    "        img_emb = model.encode(image=image_path)\n",
    "        txt_emb = model.encode(text=paragraph)\n",
    "        similarity = img_emb @ txt_emb.T\n",
    "        return similarity.item()\n",
    "\n",
    "    # Initialize counters for each evaluation condition\n",
    "    correct_matches = 0\n",
    "    total_matches = 0\n",
    "    correct_image_matches = 0\n",
    "    total_image_matches = 0\n",
    "    correct_blank_matches = 0\n",
    "    total_blank_matches = 0\n",
    "\n",
    "    all_true_labels = []\n",
    "    all_model_answers = []\n",
    "    news_item_results = []\n",
    "\n",
    "    for content in tqdm(benchmark_data):\n",
    "        image_database = content['imagedatabase']\n",
    "\n",
    "        for news_item in tqdm(content['news_text'], desc=\"News Items\"):\n",
    "            selected_images = set()\n",
    "            paragraph_content = news_item['content']\n",
    "            model_answer = [\"\" for _ in range(len(paragraph_content))]\n",
    "            true_label = news_item['groundtruth']\n",
    "            inputs_paragraphs = ''\n",
    "            news_id = news_item['id']\n",
    "\n",
    "            # 遍历每一个段落\n",
    "            for index_paragraph, paragraph in enumerate(paragraph_content):\n",
    "                # inputs_paragraphs += '\\n' + paragraph\n",
    "                inputs_paragraphs += paragraph\n",
    "                highest_similarity = -float('inf')\n",
    "                best_image_idx = None\n",
    "\n",
    "                # 遍历每一张图片\n",
    "                for index_img, img in enumerate(image_database):\n",
    "                    if index_img in selected_images:\n",
    "                        continue\n",
    "                    image_path = benchmark_root_path + '/' + img[0]\n",
    "\n",
    "                    # 计算相似度\n",
    "                    similarity = compute_similarity(inputs_paragraphs, image_path)\n",
    "\n",
    "                    if similarity > highest_similarity:\n",
    "                        highest_similarity = similarity\n",
    "                        best_image_idx = index_img\n",
    "\n",
    "                if best_image_idx is not None and highest_similarity > similarity_threshold:\n",
    "                    selected_images.add(best_image_idx)\n",
    "                    model_answer[index_paragraph] = image_database[best_image_idx][0]\n",
    "\n",
    "            all_true_labels.extend(true_label)\n",
    "            all_model_answers.extend(model_answer)\n",
    "\n",
    "            for model_answer_item, true_label_item in zip(model_answer, true_label):\n",
    "                # Calculate for all matches\n",
    "                if model_answer_item == true_label_item:\n",
    "                    correct_matches += 1\n",
    "                total_matches += 1\n",
    "\n",
    "                # Calculate only for image names\n",
    "                if true_label_item != \"\":\n",
    "                    if model_answer_item == true_label_item:\n",
    "                        correct_image_matches += 1\n",
    "                    total_image_matches += 1\n",
    "\n",
    "                # Calculate only for \"\"\n",
    "                if true_label_item == \"\":\n",
    "                    if model_answer_item == true_label_item:\n",
    "                        correct_blank_matches += 1\n",
    "                    total_blank_matches += 1\n",
    "\n",
    "            # 保存news_item的结果\n",
    "            news_item_results.append({\n",
    "                'id': news_id,\n",
    "                'true_label': true_label,\n",
    "                'model_answer': model_answer\n",
    "            })\n",
    "\n",
    "    # 计算性能指标\n",
    "    performance_metrics = {\n",
    "        'overall_accuracy': correct_matches / total_matches if total_matches > 0 else 0,\n",
    "        'image_only_accuracy': correct_image_matches / total_image_matches if total_image_matches > 0 else 0,\n",
    "        'blank_only_accuracy': correct_blank_matches / total_blank_matches if total_blank_matches > 0 else 0\n",
    "    }\n",
    "\n",
    "    # 返回结果字典和性能指标\n",
    "    return {\n",
    "        'performance_metrics': performance_metrics,\n",
    "        'news_item_results': news_item_results,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def main(\n",
    "    model_name: str,\n",
    "    model_path: str,\n",
    "    dataset_path: str,\n",
    "    results_dir: str,\n",
    "    img_path = None,\n",
    "    eval_mode: str='single_choice',\n",
    "    seed = 42,\n",
    "    similarity_threshold=0.43\n",
    "):\n",
    "    random.seed(seed)\n",
    "    # 加载benchmark数据\n",
    "    benchmark_data = load(dataset_path)\n",
    "    benchmark_root_path = os.path.dirname(os.path.abspath(dataset_path)) if img_path is None else img_path\n",
    "\n",
    "    # 初始化模型\n",
    "    if model_name == \"random\":\n",
    "        model = None\n",
    "    else:\n",
    "        model = Visualized_BGE(model_name_bge=model_name, model_weight=model_path).cuda()\n",
    "\n",
    "    # 确保结果目录存在\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # 调用验证函数进行验证\n",
    "    if eval_mode.startswith('single_choice'):\n",
    "        with torch.no_grad():\n",
    "            results_dict = eval_single_choice(benchmark_data, benchmark_root_path, model)\n",
    "        \n",
    "        json_path = os.path.join(results_dir, 'results.json')\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(results_dict, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        return results_dict\n",
    "    \n",
    "    elif eval_mode.startswith('flow_insert'):\n",
    "        with torch.no_grad():\n",
    "            results_dict = eval_flow_insert(benchmark_data, benchmark_root_path, model, similarity_threshold=similarity_threshold)\n",
    "        \n",
    "        # 保存结果为JSON文件\n",
    "        json_path = os.path.join(results_dir, 'results.json')\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(results_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        return results_dict\n",
    "    \n",
    "    elif eval_mode.startswith('fake_news'):\n",
    "        results_df = eval_fake_news(benchmark_data[:500], benchmark_root_path, model)\n",
    "\n",
    "        # 保存结果为CSV文件\n",
    "        csv_path = os.path.join(results_dir, 'results.csv')\n",
    "        results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        return results_df\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "eval_mode_list = [\n",
    "    'single_choice_1_cn',\n",
    "    'single_choice_2_cn',\n",
    "    'single_choice_3_cn',\n",
    "    'single_choice_4_cn',\n",
    "    'single_choice_1_en',\n",
    "    'single_choice_2_en',\n",
    "    'single_choice_3_en',\n",
    "    'single_choice_4_en',\n",
    "]\n",
    "\n",
    "for eval_mode in eval_mode_list:\n",
    "    model_name = \"BAAI/bge-m3\"\n",
    "    model_path = \"/PATH_TO_YOURS/models/BAAI/bge-visualized/Visualized_m3.pth\"\n",
    "    dataset_path = f'/PATH_TO_YOURS_ftii_data/newsinsertbench_{eval_mode}.tsv'\n",
    "    if eval_mode.split('_')[-1] == 'cn':\n",
    "        img_path = '/PATH_TO_YOURS/images/NewsImages_cn_jpg'\n",
    "    else: img_path = '/PATH_TO_YOURS/images'\n",
    "\n",
    "    results_dir = '/PATH_TO_YOURS/results/' + eval_mode + '/' + model_path.split('/')[-1]\n",
    "    results_dict = main(model_name = model_name,\n",
    "                        model_path = model_path,\n",
    "                        dataset_path = dataset_path,\n",
    "                        results_dir = results_dir,\n",
    "                        img_path = img_path,\n",
    "                        eval_mode = eval_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "eval_mode_list = [\n",
    "    'single_choice_1_cn',\n",
    "    'single_choice_2_cn',\n",
    "    'single_choice_3_cn',\n",
    "    'single_choice_4_cn',\n",
    "    'single_choice_1_en',\n",
    "    'single_choice_2_en',\n",
    "    'single_choice_3_en',\n",
    "    'single_choice_4_en',\n",
    "]\n",
    "\n",
    "for eval_mode in eval_mode_list:\n",
    "    model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "    model_path = \"/PATH_TO_YOURS/models/BAAI/bge-visualized/Visualized_base_en_v1.5.pth\"\n",
    "    dataset_path = f'/PATH_TO_YOURS_ftii_data/newsinsertbench_{eval_mode}.tsv'\n",
    "    if eval_mode.split('_')[-1] == 'cn':\n",
    "        img_path = '/PATH_TO_YOURS/images/NewsImages_cn_jpg'\n",
    "    else: img_path = '/PATH_TO_YOURS/images'\n",
    "\n",
    "    results_dir = '/PATH_TO_YOURS/results/' + eval_mode + '/' + model_path.split('/')[-1]\n",
    "    results_dict = main(model_name = model_name,\n",
    "                        model_path = model_path,\n",
    "                        dataset_path = dataset_path,\n",
    "                        results_dir = results_dir,\n",
    "                        img_path = img_path,\n",
    "                        eval_mode = eval_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "eval_mode_list = [\n",
    "    'flow_insert_1_cn',\n",
    "    'flow_insert_2_cn',\n",
    "    'flow_insert_3_cn',\n",
    "    'flow_insert_1_en',\n",
    "    'flow_insert_2_en',\n",
    "    'flow_insert_3_en',\n",
    "]\n",
    "\n",
    "for eval_mode in eval_mode_list:\n",
    "    model_name = \"BAAI/bge-m3\"\n",
    "    model_path = \"/PATH_TO_YOURS/models/BAAI/bge-visualized/Visualized_m3.pth\"\n",
    "    dataset_path = f'/PATH_TO_YOURS_ftii_data/newsinsertbench_{eval_mode}.json'\n",
    "    if eval_mode.split('_')[-1] == 'cn':\n",
    "        img_path = '/PATH_TO_YOURS/images/NewsImages_cn_jpg'\n",
    "    else: img_path = '/PATH_TO_YOURS/images'\n",
    "\n",
    "    results_dir = '/PATH_TO_YOURS/results/' + eval_mode + '/' + model_path.split('/')[-1]\n",
    "    results_dict = main(model_name = model_name,\n",
    "                        model_path = model_path,\n",
    "                        dataset_path = dataset_path,\n",
    "                        results_dir = results_dir,\n",
    "                        img_path = img_path,\n",
    "                        eval_mode = eval_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "eval_mode_list = [\n",
    "    'flow_insert_1_cn',\n",
    "    'flow_insert_2_cn',\n",
    "    'flow_insert_3_cn',\n",
    "    'flow_insert_1_en',\n",
    "    'flow_insert_2_en',\n",
    "    'flow_insert_3_en',\n",
    "]\n",
    "\n",
    "for eval_mode in eval_mode_list:\n",
    "    model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "    model_path = \"/PATH_TO_YOURS/models/BAAI/bge-visualized/Visualized_base_en_v1.5.pth\"\n",
    "    dataset_path = f'/PATH_TO_YOURS_ftii_data/newsinsertbench_{eval_mode}.tsv'\n",
    "    if eval_mode.split('_')[-1] == 'cn':\n",
    "        img_path = '/PATH_TO_YOURS/images/NewsImages_cn_jpg'\n",
    "    else: img_path = '/PATH_TO_YOURS/images'\n",
    "\n",
    "    results_dir = '/PATH_TO_YOURS/results/' + eval_mode + '/' + model_path.split('/')[-1]\n",
    "    results_dict = main(model_name = model_name,\n",
    "                        model_path = model_path,\n",
    "                        dataset_path = dataset_path,\n",
    "                        results_dir = results_dir,\n",
    "                        img_path = img_path,\n",
    "                        eval_mode = eval_mode)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
